{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\why\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import metrics as sk_metrics\n",
    "import pickle\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aftershock_threshold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path = './Data/days_after_mainshock_360/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_columns = ['aftershock_mag', 'sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy', 'coulomb', 'maxshear', 'von']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "add_self, add_sqrt, add_abs, add_physic, add_log = False, False, True, True, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "def get_filename(filepath = file_path):\n",
    "    file_list = []\n",
    "    for filename in os.listdir(filepath):\n",
    "        file_list.append(filename)\n",
    "    return file_list\n",
    "file_list = get_filename()\n",
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mag_range = [3.0, 5.0, 5.5, 6.0, 8.0]\n",
    "dis_range = [0, 20, 40, 60, 100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['x', 'y', 'mainshock_mag', 'aftershock_mag', 'sxx', 'syy', 'szz', 'syz',\n",
      "       'sxz', 'sxy', 'coulomb', 'maxshear', 'von'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "tmp = pd.read_csv(file_path+file_list[0],index_col=0)\n",
    "print(tmp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sample(file_list, col_list, filepath=file_path, add_self=add_self, add_sqrt=add_sqrt, add_abs=add_abs, \n",
    "               add_physic=add_physic,add_log=add_log):\n",
    "    new_col = ['aftershock_mag']\n",
    "    if add_self:\n",
    "        for col in col_list[1::]:\n",
    "            new_col.append(col)\n",
    "    if add_abs:\n",
    "        for col in col_list[1::]:\n",
    "            if col in ['sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy']:\n",
    "                new_col.append(\"abs_\"+col)\n",
    "    if add_sqrt:\n",
    "        for col in col_list[1::]:\n",
    "            if col in ['sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy']:\n",
    "                new_col.append(\"sqrt_\"+col)\n",
    "    if add_physic:\n",
    "        for col in col_list[1::]:\n",
    "            if col in ['coulomb', 'maxshear', 'von']:\n",
    "                new_col.append('abs_'+col)\n",
    "    if add_log:\n",
    "        for col in col_list[1::]:\n",
    "            if col in ['sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy']:\n",
    "                new_col.append(\"log_\"+col)            \n",
    "    raw_data = None  \n",
    "    for file in file_list:\n",
    "        main_Mag = float(file.strip('.csv').split('-')[1][1::])\n",
    "        df = pd.read_csv(filepath+file, header=0)\n",
    "        axis = np.where(np.isnan(df))\n",
    "        if len(axis[0]) != 0:\n",
    "            print(\"%s is wrong\" % file)\n",
    "            continue\n",
    "        unique = df['aftershock_mag'].unique()\n",
    "        if(len(unique)==1):\n",
    "            continue\n",
    "        \n",
    "        index = len(df.columns)\n",
    "        columns = df.columns.copy()\n",
    "        for col in columns:\n",
    "            df.insert(index, '-'+col, -df[col])\n",
    "            df.insert(index+1, 'abs_'+col, abs(df[col]))\n",
    "            df.insert(index+2, '-abs_'+col, -abs(df[col]))\n",
    "            col_sqrt = df[col].apply(lambda x: math.sqrt(abs(x)))\n",
    "            df.insert(index+3, 'sqrt_'+col, col_sqrt)\n",
    "            df.insert(index+4, '-sqrt_'+col, -col_sqrt)\n",
    "            col_log = df[col].apply(lambda x: np.log(abs(x)+1))\n",
    "            df.insert(index+5, 'log_'+col, col_log)\n",
    "            df.insert(index+6, '-log_'+col, -col_log)\n",
    "            \n",
    "        for idx in range(len(mag_range) - 1):\n",
    "            if main_Mag >= mag_range[idx] and main_Mag < mag_range[idx+1]:\n",
    "                break\n",
    "        mainMag_oneHot = [[0]*(len(mag_range)-1)]\n",
    "        mainMag_oneHot[0][idx] = 1\n",
    "        mainMag_oneHot = mainMag_oneHot * len(df)\n",
    "        index = len(df.columns)\n",
    "        mainMag_oneHot = np.array(mainMag_oneHot)\n",
    "        for idx in range(len(mag_range)-1):\n",
    "            df.insert(index+idx, 'mainMag_oneHot_'+str(idx), mainMag_oneHot[:, idx])\n",
    "        \n",
    "        dis_oneHot = []\n",
    "        for x, y in zip(df['x'].values, df['y'].values):\n",
    "#             print(x, y)\n",
    "            dis = math.sqrt(x*x + y*y)\n",
    "            for idx in range(len(dis_range)-1):\n",
    "                if dis >= dis_range[idx] and dis < dis_range[idx+1]:\n",
    "                    break\n",
    "            dis_oneHot_tmp = [0]*(len(dis_range)-1)\n",
    "            dis_oneHot_tmp[idx] = 1\n",
    "            dis_oneHot.append(dis_oneHot_tmp)\n",
    "\n",
    "        index = len(df.columns)\n",
    "        dis_oneHot = np.array(dis_oneHot)\n",
    "        for idx in range(len(dis_range)-1):\n",
    "            df.insert(index+idx, 'dis_oneHot_'+str(idx), dis_oneHot[:, idx])\n",
    "        if raw_data is None:\n",
    "            raw_data = df\n",
    "        else:\n",
    "            raw_data = raw_data.append(df)\n",
    "    \n",
    "    if raw_data is None:\n",
    "        return raw_data\n",
    "    if len(raw_data) == 0:\n",
    "        return raw_data[new_col]\n",
    "\n",
    "    for idx in range(len(mag_range)-1):\n",
    "        new_col.append('mainMag_oneHot_'+str(idx))\n",
    "    for idx in range(len(dis_range)-1):\n",
    "        new_col.append('dis_oneHot_'+str(idx))\n",
    "    \n",
    "    raw_data = raw_data[new_col]\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_data = get_sample(file_list[0:5], feat_columns, file_path, add_self=add_self, add_sqrt=add_sqrt, add_abs=add_abs, add_physic=add_physic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aftershock_mag', 'abs_sxx', 'abs_syy', 'abs_szz', 'abs_syz', 'abs_sxz',\n",
      "       'abs_sxy', 'abs_coulomb', 'abs_maxshear', 'abs_von', 'log_sxx',\n",
      "       'log_syy', 'log_szz', 'log_syz', 'log_sxz', 'log_sxy',\n",
      "       'mainMag_oneHot_0', 'mainMag_oneHot_1', 'mainMag_oneHot_2',\n",
      "       'mainMag_oneHot_3', 'dis_oneHot_0', 'dis_oneHot_1', 'dis_oneHot_2',\n",
      "       'dis_oneHot_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\why\\Anaconda3\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(POSdata, NEGdata, batch_size, posstart, negstart):\n",
    "    shapepos = np.shape(POSdata)\n",
    "    shapeneg = np.shape(NEGdata)\n",
    "    np.random.shuffle(POSdata)\n",
    "    np.random.shuffle(NEGdata)\n",
    "    ratio = round(shapeneg[0] / shapepos[0])\n",
    "#     ratio = 1\n",
    "    batch_posCnt = round(batch_size / (1 + ratio))\n",
    "    batch_negCnt = batch_size - batch_posCnt\n",
    "    while 1:\n",
    "        if posstart + batch_posCnt >= shapepos[0]: \n",
    "            posstart = 0\n",
    "            np.random.shuffle(POSdata)\n",
    "        posend = posstart + batch_posCnt\n",
    "        if negstart + batch_negCnt >= shapeneg[0]: \n",
    "            negstart = 0\n",
    "            np.random.shuffle(NEGdata)\n",
    "        negend = negstart + batch_negCnt       \n",
    "        data = np.row_stack((POSdata[posstart:posend,:], NEGdata[negstart:negend,:]))\n",
    "        np.random.shuffle(data)\n",
    "        posstart = posstart + batch_posCnt\n",
    "        negstart = negstart + batch_negCnt\n",
    "        yield (data[:,1:-1], data[:,0], data[:, -1])     #   feat, label, weight       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.lr = 0.1\n",
    "        self.train_batch_size = 128 * 2\n",
    "        self.eval_batch_size = 128\n",
    "        self.test_batch_size = 128\n",
    "        self.num_epoch = 100\n",
    "        self.drop_rate = 0.5\n",
    "        self.feat_size = (len(raw_data.columns) - 1)\n",
    "        self.hidden_units = [64]*5\n",
    "        self.embedding_size = 16\n",
    "        self.max_grad_norm = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(object):\n",
    "    def __init__(self, is_training, args):\n",
    "        self.train_batch_size = args.train_batch_size\n",
    "        self.eval_batch_size = args.eval_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "        self.num_epoch = args.num_epoch\n",
    "        self.drop_rate = args.drop_rate\n",
    "        \n",
    "        self.hidden_units = args.hidden_units\n",
    "        self.max_grad_norm = args.max_grad_norm\n",
    "        self.lr = args.lr\n",
    "        \n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.feat_size = args.feat_size\n",
    "        \n",
    "        self.input_data = tf.placeholder(tf.float32, [None, self.feat_size])\n",
    "        self.targets = tf.placeholder(tf.float32, [None])\n",
    "        self.sample_weight = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        with tf.variable_scope(\"Embedding_Mag\", reuse=tf.AUTO_REUSE):\n",
    "            embedding_mag = tf.get_variable(\"embedding_mag\", [len(mag_range)-1, self.embedding_size])\n",
    "        \n",
    "        with tf.variable_scope(\"Embedding_Dis\", reuse=tf.AUTO_REUSE):\n",
    "            embedding_dis = tf.get_variable(\"embedding_dis\", [len(dis_range)-1, self.embedding_size])\n",
    "            \n",
    "        feat_size_true = 6*1\n",
    "        with tf.variable_scope(\"Embedding_Stress\", reuse=tf.AUTO_REUSE):\n",
    "            embedding_stress = tf.get_variable(\"embedding_stress\", [feat_size_true, self.embedding_size])\n",
    "        \n",
    "        stress_cnt = 6*1\n",
    "        stress_sqrt_cnt = 6*1\n",
    "        phisics_cnt = 3*1\n",
    "        log_cnt = 6*1\n",
    "        mainMag_cnt = len(mag_range)-1\n",
    "        dis_cnt = len(dis_range)-1\n",
    "        \n",
    "        \n",
    "        stress_feat = self.input_data[:, 0:stress_cnt]\n",
    "        phisics_feat = self.input_data[:, stress_cnt:(stress_cnt+phisics_cnt)]\n",
    "        stress_log_feat = self.input_data[:, (stress_cnt+phisics_cnt):(stress_cnt+phisics_cnt+log_cnt)]\n",
    "\n",
    "        mainMag_feat = self.input_data[:, (stress_cnt+phisics_cnt+log_cnt):(stress_cnt+phisics_cnt+log_cnt+mainMag_cnt)]\n",
    "        dis_feat = self.input_data[:, (stress_cnt+phisics_cnt+log_cnt+mainMag_cnt)::]\n",
    "        \n",
    "        \n",
    "        \n",
    "        feat_in = tf.concat([stress_feat, stress_log_feat, phisics_feat[:,1::], mainMag_feat, dis_feat], axis=1)\n",
    "        with tf.variable_scope(\"LR\", reuse=tf.AUTO_REUSE):\n",
    "            self.logits = tf.layers.dense(inputs=feat_in, units=1, activation=None,\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(0.003))\n",
    "        \n",
    "        targets = tf.expand_dims(self.targets, -1)\n",
    "        targets_binary = tf.cast(targets > 0, tf.float32)\n",
    "        self.predict_prob = tf.nn.sigmoid(self.logits)\n",
    "                \n",
    "        if not is_training:\n",
    "            return\n",
    "        cross_loss = (tf.nn.sigmoid_cross_entropy_with_logits(logits=self.logits, labels=targets_binary))\n",
    "        sample_weight = tf.expand_dims(self.sample_weight, -1)\n",
    "        self.loss = tf.reduce_sum((cross_loss * sample_weight) / tf.reduce_sum(self.sample_weight))\n",
    "        \n",
    "        self.tot_loss = self.loss\n",
    "\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.tot_loss, trainable_variables), self.max_grad_norm)\n",
    "        self.learning_rate=tf.Variable(float(self.lr), trainable=False)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\n",
    "        \n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(self.learning_rate * 0.99)\n",
    "        self.saver=tf.train.Saver(tf.global_variables(), max_to_keep=1)\n",
    "#         self.saver=tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run(scope, train_pos, train_neg, eval_data, feat_size):\n",
    "    args = Args()\n",
    "    args.feat_size = feat_size\n",
    "    print('feat_size: %d' % args.feat_size)\n",
    "    train_neg_num = np.shape(train_neg)[0]\n",
    "    res_evalLabel = []\n",
    "    res_predictProb = []\n",
    "    \n",
    "    with tf.variable_scope(\"Model_\"+scope, reuse=None):\n",
    "        train_model = model(is_training=True, args=args)\n",
    "    with tf.variable_scope(\"Model_\"+scope, reuse=True):\n",
    "        eval_model = model(is_training=False, args=args)\n",
    "    with tf.Session() as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        tot_loss = 0.\n",
    "        posstart = 0\n",
    "        negstart = 0\n",
    "        pre_loss = []\n",
    "        \n",
    "        cnt = 0\n",
    "        auc_list = []\n",
    "        for step, (train_feat, train_label, sample_weight) in enumerate(\n",
    "            generate_data(train_pos, train_neg, args.train_batch_size, posstart, negstart)):\n",
    "            \n",
    "            mask_train = (train_label>=aftershock_threshold).astype(int)\n",
    "            train_label = train_label * mask_train\n",
    "            \n",
    "            loss, _ = sess.run([train_model.loss, train_model.train_op],\n",
    "                              feed_dict={train_model.input_data: train_feat,\n",
    "                                         train_model.targets: train_label,\n",
    "                                         train_model.sample_weight:sample_weight})\n",
    "            tot_loss += loss\n",
    "            num_steps = train_neg_num // args.train_batch_size\n",
    "            if(step % num_steps == 0):\n",
    "                cnt += 1\n",
    "                print(tot_loss)\n",
    "                tot_loss = 0\n",
    "              \n",
    "                eval_feat = eval_data.values[:, 1::]\n",
    "                \n",
    "                eval_label = np.array(eval_data.values[:, 0])\n",
    "                mask_eval = (eval_label>=aftershock_threshold).astype(int)\n",
    "                eval_label = eval_label * mask_eval\n",
    "                eval_label_binary = mask_eval\n",
    "                \n",
    "                predict_prob = sess.run([eval_model.predict_prob],\n",
    "                                             feed_dict = {\n",
    "                                                  eval_model.input_data: eval_feat,\n",
    "                                                  eval_model.targets: eval_label, \n",
    "                                             })\n",
    "                \n",
    "                predict_prob = np.reshape(predict_prob, -1)     \n",
    "                auc = sk_metrics.roc_auc_score(eval_label_binary, predict_prob)\n",
    "                print(\"epoch: %d auc:%.4lf\" % (cnt,auc))\n",
    "                \n",
    "                if len(auc_list)==0 or auc>max(auc_list):\n",
    "                    res_evalLabel = eval_label\n",
    "                    res_predictProb = predict_prob\n",
    "                auc_list.append(auc)\n",
    "                \n",
    "            if cnt >= 15:\n",
    "                return max(auc_list), res_evalLabel, res_predictProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3. Train Model and Cross Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    }
   ],
   "source": [
    "print(len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82, 48, 61, 69, 92, 164, 145, 7, 156, 117, 80, 144, 146, 123, 78, 6, 53, 103, 36, 95, 150, 1, 50, 81, 172, 88, 126, 51, 27, 143, 12, 45, 101, 136, 2, 132, 16, 170, 157, 115, 30, 151, 83, 90, 35, 56, 168, 58, 153, 24, 32, 79, 67, 89, 169, 65, 152, 133, 158, 137, 100, 33, 106, 4, 91, 166, 54, 140, 163, 85, 13, 139, 141, 75, 174, 44, 21, 23, 14, 110, 118, 77, 73, 28, 66, 76, 70, 148, 119, 161, 63, 20, 11, 160, 34, 98, 62, 40, 108, 64, 57, 165, 155, 52, 68, 18, 43, 125, 49, 105, 112, 128, 71, 15, 130, 93, 74, 167, 97, 96, 147, 114, 134, 86, 26, 25, 99, 19, 171, 120, 9, 47, 38, 124, 102, 87, 41, 116, 142, 31, 22, 39, 5, 159, 29, 127, 138, 84, 8, 113, 104, 111, 10, 154, 59, 37, 17, 109, 149, 94, 3, 0, 60, 135, 129, 46, 131, 121, 55, 122, 72, 162, 173, 107, 42]\n"
     ]
    }
   ],
   "source": [
    "cnt = len(file_list)\n",
    "Index = [i for i in range(cnt)]\n",
    "random.seed(21)\n",
    "random.shuffle(Index)\n",
    "print(Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['aftershock_mag', 'abs_sxx', 'abs_syy', 'abs_szz', 'abs_syz', 'abs_sxz',\n",
      "       'abs_sxy', 'abs_coulomb', 'abs_maxshear', 'abs_von', 'log_sxx',\n",
      "       'log_syy', 'log_szz', 'log_syz', 'log_sxz', 'log_sxy',\n",
      "       'mainMag_oneHot_0', 'mainMag_oneHot_1', 'mainMag_oneHot_2',\n",
      "       'mainMag_oneHot_3', 'dis_oneHot_0', 'dis_oneHot_1', 'dis_oneHot_2',\n",
      "       'dis_oneHot_3'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(raw_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftershock_mag', 'sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy', 'coulomb', 'maxshear', 'von']\n"
     ]
    }
   ],
   "source": [
    "print(feat_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DCG(label_list):\n",
    "    dcgsum = 0\n",
    "    for i in range(len(label_list)):\n",
    "        dcg = (2**label_list[i] - 1)/math.log(i+2, 2)\n",
    "        dcgsum += dcg\n",
    "    return dcgsum\n",
    "\n",
    "def NDCG(label_list):\n",
    "    dcg = DCG(label_list)\n",
    "    ideal_list = sorted(label_list, reverse=True)\n",
    "    ideal_dcg = DCG(ideal_list)\n",
    "    if ideal_dcg == 0:\n",
    "        return 0\n",
    "    return dcg/ideal_dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aftershock_mag', 'sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy', 'coulomb', 'maxshear', 'von']\n",
      "------------columns------------\n",
      "['aftershock_mag', 'sxx', 'syy', 'szz', 'syz', 'sxz', 'sxy', 'coulomb', 'maxshear', 'von']\n",
      "\n",
      "\n",
      "\n",
      "20151207-M7.2.csv is wrong\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    fold_cnt = 5\n",
    "    print(feat_columns)      \n",
    "    res = []\n",
    "    Test_label_list, Test_predictProb_list = [], []\n",
    "    NDCG_ALL = []\n",
    "    columns=feat_columns\n",
    "    print(\"------------columns------------\")\n",
    "    print(columns)\n",
    "    print(\"\\n\\n\")\n",
    "    scope = \"LR\"\n",
    "    AUC_list = []\n",
    "    NDCG_list = []\n",
    "\n",
    "    for fold in range(fold_cnt):\n",
    "        group_cnt = len(file_list) // fold_cnt\n",
    "        eval_Index = Index[fold*group_cnt:(fold+1)*group_cnt]\n",
    "        train_Index = [idx for idx in Index if idx not in eval_Index]\n",
    "\n",
    "        train_file = [file_list[idx] for idx in train_Index]\n",
    "        tmp_file = train_file.copy()\n",
    "\n",
    "        eval_file = [file_list[idx] for idx in eval_Index]\n",
    "\n",
    "        train_data = get_sample(train_file, columns)\n",
    "        eval_data = get_sample(eval_file, columns)\n",
    "\n",
    "        avg_list, std_list = [], []\n",
    "        new_columns = train_data.columns.values\n",
    "\n",
    "        print(\"columns:\", new_columns)\n",
    "\n",
    "        # Feature normalize\n",
    "        for col in new_columns[1:-(len(mag_range)-1+len(dis_range)-1)]:\n",
    "            avg = np.mean(train_data[col].values)\n",
    "            std = np.std(train_data[col].values)\n",
    "            train_data[col] = (train_data[col]-avg) / std\n",
    "            avg_list.append(avg)\n",
    "            std_list.append(std)            \n",
    "        for col, avg, std in zip(new_columns[1:-(len(mag_range)-1+len(dis_range)-1)], avg_list, std_list):\n",
    "            eval_data[col] = (eval_data[col]-avg) / std\n",
    "\n",
    "        # Add weight to each sample using the aftershock magnitude\n",
    "        weight = train_data['aftershock_mag'].apply(lambda x: 1.0)\n",
    "#         weight = train_data['aftershock_mag'].apply(lambda x:(1+x))\n",
    "        train_data['weight'] = weight\n",
    "\n",
    "        print('train_data columns:', train_data.columns)\n",
    "        pos_train_data = train_data[train_data['aftershock_mag']>=aftershock_threshold]\n",
    "        neg_train_data = train_data[train_data['aftershock_mag']<aftershock_threshold]\n",
    "\n",
    "        train_pos = pos_train_data.values\n",
    "        train_neg = neg_train_data.values\n",
    "\n",
    "\n",
    "        AUC, Test_label, Test_predictProb = run(scope, train_pos, train_neg, eval_data, feat_size=len(new_columns)-1)\n",
    "\n",
    "\n",
    "        # Calculate NDCG\n",
    "        valid_subEarth_Mag, valid_subEarth_Prob = [], []\n",
    "        for subEarth_Mag, predictProb in zip(Test_label, Test_predictProb):\n",
    "            if subEarth_Mag != 0:\n",
    "                valid_subEarth_Mag.append(subEarth_Mag)\n",
    "                valid_subEarth_Prob.append(predictProb)\n",
    "\n",
    "        valid_subEarth_Mag = np.array(valid_subEarth_Mag)\n",
    "        valid_subEarth_Prob = np.array(valid_subEarth_Prob)\n",
    "        sorted_subEarth_Mag = valid_subEarth_Mag[valid_subEarth_Prob.argsort()]\n",
    "        sorted_subEarth_Mag = list(reversed(sorted_subEarth_Mag))\n",
    "\n",
    "\n",
    "        ndcg = NDCG(sorted_subEarth_Mag)\n",
    "        NDCG_list.append(ndcg)\n",
    "\n",
    "        sorted_subEarth_Prob = sorted(valid_subEarth_Prob, reverse=True)\n",
    "\n",
    "        print(\"fold: %d, auc: %.4lf\" % (fold,AUC))\n",
    "        AUC_list.append(AUC)\n",
    "        Test_label_list.append(Test_label)\n",
    "        Test_predictProb_list.append(Test_predictProb)\n",
    "\n",
    "    res.append(AUC_list)\n",
    "    NDCG_ALL.append(NDCG_list)\n",
    "    \n",
    "    return res, Test_label_list, Test_predictProb_list, NDCG_ALL\n",
    "        \n",
    "\n",
    "res, Test_label_list, Test_predictProb_list, NDCG_ALL = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Average AUC is %.4lf\" % np.mean(res[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Output the AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the ROC curve\n",
    "\n",
    "k_fold = 5\n",
    "for index in range(k_fold):\n",
    "    label_list = []\n",
    "    predictProb_list = []\n",
    "    cur = index\n",
    "    while cur < len(Test_label_list):\n",
    "        fold_label = Test_label_list[cur]\n",
    "        fold_label = (np.array(fold_label)>0).astype(int).tolist()\n",
    "        label_list.append(fold_label)\n",
    "        predictProb_list.append(Test_predictProb_list[cur])\n",
    "        cur += k_fold\n",
    "    \n",
    "#     print(np.shape(label_list))\n",
    "#     print(np.shape(predictProb_list))\n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    feat_name = ['LR']\n",
    "    for idx, label, predictProb in zip(range(len(feat_columns)), label_list, predictProb_list):\n",
    "        \n",
    "        fpr, tpr, threshold = sk_metrics.roc_curve(label, predictProb)\n",
    "#         roc_auc = sk_metrics.auc(fpr, tpr)\n",
    "        roc_auc = sk_metrics.roc_auc_score(label, predictProb)\n",
    "    \n",
    "        plt.plot(fpr, tpr,lw=0.5, label=('AUC = %0.4f ' %  roc_auc) + feat_name[idx])\n",
    "        plt.legend(loc = 'lower right')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "\n",
    "    plt.show()\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
